---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2020, Hyunjoon Rhee"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---


# Directions

Students are encouraged to work together on homework. However, sharing, copying or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible.

- Be sure to remove this section if you use this `.Rmd` file as a template.
- You may leave the questions in your final document.

***

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
cats = MASS::cats
catsmodel = lm(Hwt ~ Bwt, data = cats)
summary(catsmodel)
```
- Null hypothesis: There is no significant relationship between Hwt and Bwt, Alternative hypothesis: There is a significant relationship between Hwt and Bwt
- $\hat{\beta}_1$ T value = 16.119, $\hat{\beta}_0$ T value = -0.515
- p-value: <2e-16
- Because $\alpha = 0.05$ and p-value is way smaller than alpha, it should reject the null hypothesis
- In conclusion because we reject the null hypothesis, it can be told that there is a signficant relationship between Hwt and Bwt

**(b)** Calculate a 95% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.
```{r}
range = confint(catsmodel, level=0.95)[2,]
```
It could be said that for 1kg increase of body weight, it is 95% likely that the heart weight will increase in a range of 3.54 and 4.53kg.


**(c)** Calculate a 90% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

```{r}
range = confint(catsmodel, level=0.90)[1,]
```
It could be said that the range of the mean of $\beta_0$ is in between the range of -1.5 and 0.79 by a chance of 90%.


**(d)** Use a 90% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

```{r}
newdata = data.frame(Bwt = c(2.1, 2.8))
interv = predict(catsmodel, newdata = newdata, interval = c("confidence"), level = 0.90)
interv
interv[1,3] - interv[1,2]
interv[2,3] - interv[2,2]
```
Body weight of 2.1 has wider interval.The range of the upper and the lower is greater considering the mean of each.

**(e)** Use a 90% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

```{r}
newdata = data.frame(Bwt = c(2.8, 4.2))
predict(catsmodel, newdata = newdata, interval = c("confidence"), level = 0.90)
```
[10.73584,11.14158]; [15.94268, 17.23012]

**(f)** Create a scatterplot of the data. Add the regression line, 95% confidence bands, and 95% prediction bands.

```{r}
new_data = seq(min(cats$Bwt), max(cats$Bwt), by = 0.1)
confband = predict(catsmodel, newdata = data.frame(Bwt = new_data), interval = c("confidence"), level = 0.95)
predband = predict(catsmodel, newdata = data.frame(Bwt = new_data), interval = c("prediction"), level = 0.95)
plot(Hwt ~ Bwt, data = cats,
     xlab = "Body Weight",
     ylab = "Heart Weight",
     main = "Heart Weight vs Body Weight",
     pch  = 20,
     cex  = 2,
     col  = "grey")
abline(catsmodel, lwd = 4, col = "darkorange")
lines(new_data, confband[,"lwr"], col = "blue", lwd = 3, lty = 2)
lines(new_data, confband[,"upr"], col = "blue", lwd = 3, lty = 2)
lines(new_data, predband[,"lwr"], col = "blue", lwd = 3, lty = 2)
lines(new_data, predband[,"upr"], col = "blue", lwd = 3, lty = 2)

points(mean(cats$Bwt), mean(cats$Hwt), pch = "*", cex = 3)
```


**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

- The value of the test statistic
```{r}
beta1 = 4
b1_hat_se = coef(summary(catsmodel))[2,2]
b1_hat = coef(summary(catsmodel))[2,1]

tvalue = (b1_hat-beta1)/(b1_hat_se)
```
- The p-value of the test
```{r}
pvalue = 2*pt(tvalue, df=nrow(cats)-2, lower.tail=FALSE)
```
- A statistical decision at $\alpha = 0.05$
p value is greater than alpha, which means that it fails to reject the null hypothesis that $\beta_1 = 4$.

***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

```{r}
ozone_wind_model = lm(ozone~wind, data = Ozone)
summary(ozone_wind_model)
```
Null hypothesis: There is no significant relationship between ozone measurement and wind speed
Alternative hypothesis: There is significant relationship between ozone measurement and wind speed

```{r}
tvalue2 = coef(summary(ozone_wind_model))[2,3]
pvalue2 = coef(summary(ozone_wind_model))[2,4]
```
Since pvalue2 > alpha, it fails to reject the null hypothesis. Therefore there is no significant relationship between ozone and wind speed.


**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.
```{r}
ozone_temp_model = lm(ozone~temp, data = Ozone)
summary(ozone_temp_model)
```
Null hypothesis: There is no significant relationship between ozone measurement and temp
Alternative hypothesis: There is significant relationship between ozone measurement and temp

```{r}
tvalue3 = coef(summary(ozone_temp_model))[2,3]
pvalue3 = coef(summary(ozone_temp_model))[2,4]
```
Since pvalue3 < alpha, it rejects the null hypothesis. Therefore there is significant relationship between ozone and wind speed.

***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.
```{r}
sim_func = function(x, beta_0, beta_1, sigma) {
  n = length(x)
  ep = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + ep
  data.frame(predictor = x, response = y)
}
```

```{r}
birthday = 19960926
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)

beta_0 = -5
beta_1 = 3.25
sigma = 4
beta_1_hat=rep(0,2000)
beta_0_hat=rep(0,2000)
Sxx=sum((x-mean(x))^2)
for(i in 1:2000) {
  data3 = sim_func(x, beta_0, beta_1, sigma)
  model3 = lm(response ~ predictor, data = data3)
  beta_0_hat[i] = coef(model3)[1]
  beta_1_hat[i] = coef(model3)[2]
}
```

**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values

```{r}
var0 = sigma^2 * (1/n + (mean(x)^2 / Sxx))
var1 = sigma^2 / Sxx

col1 = c("True expected value" = beta_0, "Mean of simulated values" = mean(beta_0_hat), "True standard deviation" = sqrt(var0), "Simulated standard deviation" = sd(beta_0_hat))
col2 = c("True expected value" = beta_1, "Mean of simulated values" = mean(beta_1_hat), "True standard deviation" = sqrt(var1), "Simulated standard deviation" = sd(beta_1_hat))
table = data.frame(beta_0_hat = col1, beta_1_hat = col2)
knitr::kable(table, row.names = TRUE)
```


**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.

```{r}
hist(beta_0_hat, prob = TRUE, breaks = 25, 
     xlab = expression(hat(beta)[0]),
     main = "",
     border = "blue"
     )
curve(dnorm(x, mean = beta_0, sd = sqrt(var0)), col = "orange", add = TRUE, lwd = 3)
```


- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.

```{r}
hist(beta_1_hat, prob = TRUE, breaks = 25, 
     xlab = expression(hat(beta)[1]),
     main = "",
     border = "blue"
     )
curve(dnorm(x, mean = beta_1, sd = sqrt(var1)), col = "orange", add = TRUE, lwd = 2)
```


***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19960926
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)

beta_0 = 5
beta_1 = 2
sigma = 3
beta_1_hat4=rep(0,2500)
beta_0_hat4=rep(0,2500)
Sxx=sum((x-mean(x))^2)
s_e = rep(0, 2500)
for(i in 1:2500) {
  data4 = sim_func(x, beta_0, beta_1, sigma)
  model4 = lm(response ~ predictor, data = data4)
  beta_0_hat4[i] = coef(model4)[1]
  beta_1_hat4[i] = coef(model4)[2]
  s_e[i] = summary(model4)$coefficients[2, 2]
}
```

**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

```{r}
confint(model4, level = 0.95)
cval = qt(.025, df=n-2, lower.tail = FALSE)
lower_95 = beta_1_hat4 - cval * s_e
upper_95 = beta_0_hat4 + cval * s_e
intervals = data.frame("lower" = lower_95, "upper" = upper_95);
#intervals
```


**(c)** What proportion of these intervals contains the true value of $\beta_1$?

```{r}
mean((lower_95 <= beta_1) & (beta_1 <= upper_95))
```


**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?

```{r}
1-mean((lower_95 <= beta_1) & (beta_1 <= upper_95))
```


**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

```{r}
confint(model4, level = 0.99)
cval99 = qt(.005/2, df=n-2, lower.tail = FALSE)
lower_99 = beta_1_hat4 - cval99 * s_e
upper_99 = beta_0_hat4 + cval99 * s_e
intervals = data.frame("lower" = lower_99, "upper" = upper_99);
#intervals
```


**(f)** What proportion of these intervals contains the true value of $\beta_1$?

```{r}
mean((lower_99 <= beta_1) & (beta_1 <= upper_99))
```


**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?

```{r}
1- mean((lower_95 <= beta_1) & (beta_1 <= upper_95))
```


***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval


```{r}
calc_pred_int = function(model, newdata, level=.95) {
  n = nrow(model$model)
  y_hat = predict(model, newdata = newdata)
  columns = colnames(newdata)
  xmean = mean(model$model[,columns])
  alpha = 0.05
  cval = qt(alpha/2, df=n-2, lower.tail = FALSE)
  sigma = summary(model)$sigma
  Sxx = sum((model$model[,columns] -xmean)^2)
  se = sqrt(sigma^2 * (1 + 1/n +((newdata-xmean)^2/Sxx)))
  lower95 = (y_hat - cval*se)[,1]
  upper95 = (y_hat + cval*se)[,1]
  return (c(estimate=y_hat, lower=lower95, upper=upper95))
}
```


**(b)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(catsmodel, newcat_1)
```

**(c)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(catsmodel, newcat_2, level = 0.90)
```


